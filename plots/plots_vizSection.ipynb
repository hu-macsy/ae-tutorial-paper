{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import aextl\n",
    "import numpy as np\n",
    "import pandas\n",
    "import os\n",
    "import yaml\n",
    "import math\n",
    "from collectResults import *\n",
    "#import collectResults\n",
    "from operator import itemgetter\n",
    "from IPython.display import display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#probably, the next is not needed\n",
    "#%run collectResults.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configFile = \"../experiments/\"\n",
    "\n",
    "allRuns = list()\n",
    "#allRuns: an object containing all runs\n",
    "#cfg: object returned from the aext tool\n",
    "allRuns, cfg = collectAllRuns( configFile )\n",
    "\n",
    "#TODO: add all required information in allRuns so cfg is not needed\n",
    "#TODO2: then why not use cfg in the first place?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test: print run times for a specific experiment for a specific instance\n",
    "\n",
    "# 22/05: there are data for rk and dimacs9-BAY but this graph is not in the evaluation set so the data are not gathered\n",
    "#print( [run['run_time'] for run in allRuns if (run['experiment']=='rk' and run['instance']=='dimacs9-COL') ] )\n",
    "#print( [run['run_time'] for run in allRuns if (run['experiment']=='kadabra' and run['instance']=='dimacs9-COL') ] )\n",
    "\n",
    "print( 'possible experiment are:' , set([run['experiment'] for run in allRuns]) )\n",
    "\n",
    "print( \"possible parameter values:\\n\", list(dict.fromkeys([run['parameters']['iters_per_step'] for run in allRuns if run['instance']=='advogato' and 'iters_per_step' in run['parameters'] ] ) ))\n",
    "\n",
    "#print( [run['variation'] for run in allRuns if run['instance']=='advogato' and 'iters_per_step' in run['parameters'] ] )\n",
    "\n",
    "print( allRuns[134] ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lexicographic sorting\n",
    "\n",
    "#sort first by experiment/tool name and then by instance name\n",
    "#probably not needed but better to be sure than sorry\n",
    "allRuns.sort( key=itemgetter( 'experiment', 'instance') )\n",
    "\n",
    "print(\"All runs are\", len(allRuns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add the number of nodes and edges of the instance for every run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: if a subset of graphs is stored in the file, it will read only from the file \n",
    "# and ignore the new graphs\n",
    "_ = addAttributes( allRuns )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store instances attributes to file \n",
    "#storeInstToFile( allRuns, 'instancesData.yml' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check if we have data for all input instances\n",
    "If not, remove the runs that do not contain data for both algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toBeRemoved = list()\n",
    "\n",
    "for run in allRuns:\n",
    "    inst = run['instance']\n",
    "    algo = run['experiment']\n",
    "    flag = True\n",
    "    for run2 in allRuns:\n",
    "        #there is a run for the same instance but different algorithm\n",
    "        if run2['instance']==inst and run2['experiment']!=algo:\n",
    "            flag = False\n",
    "            break\n",
    "    if flag:\n",
    "        #print( run )\n",
    "        #print( run2 )\n",
    "        toBeRemoved.append( inst )\n",
    "\n",
    "toBeRemoved = list(dict.fromkeys(toBeRemoved))        \n",
    "print( \"instances to be removed:\", toBeRemoved) \n",
    "print(\"WARNING: this is the evaluation set for which we do not collect rk runs; do not remove\")\n",
    "\n",
    "#do not remove anything\n",
    "toBeRemoved.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmpFilterRuns = list()\n",
    "runsRemoved = 0\n",
    "for run in allRuns:\n",
    "    if len(run)!=0 and run['instance'] not in toBeRemoved:\n",
    "        tmpFilterRuns.append( run )\n",
    "        #run.clear()\n",
    "    else:\n",
    "        runsRemoved+=1\n",
    "        #print(run['instance'])\n",
    "            \n",
    "print( \"runs removed are\" , runsRemoved )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allRuns.clear()\n",
    "allRuns = tmpFilterRuns.copy()\n",
    "print( len(allRuns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seperate runs that belong to the tunning set and to the evaluation test\n",
    "Tunning set contains only runs for kadabra (this is the algorithm we tune).\n",
    "The two sets have some overlap, that is, the runs for the tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all parameter values\n",
    "allParams = list()\n",
    "\n",
    "for run in allRuns:\n",
    "    if 'iters_per_step' in run['parameters']:\n",
    "        allParams.append( run['parameters']['iters_per_step'] )\n",
    "        allParams = list( dict.fromkeys(allParams) )\n",
    "        \n",
    "# turn None to -1\n",
    "for i in range(len(allParams)):\n",
    "    if allParams[i]==None:\n",
    "        allParams[i]=-1\n",
    "print(\"all available parameter values are:\", allParams)\n",
    "print(\"RK does not have a parameter; we set a dummy value to -1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the tunning parameter value\n",
    "# this is determined after the tunning phase\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../experiments/')\n",
    "#%run ../experiments/explore.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from explore import *\n",
    "#calculates and prints the tunning parameter\n",
    "tunedParam, minTuningTime = getTunParam()\n",
    "minTuningTime = round( minTuningTime, 1 )\n",
    "print(\"tuned parameter value is \" + str(tunedParam) + \", min time is \" + str(minTuningTime) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter all runs by parameter value\n",
    "filteredRunsPerParam = dict()\n",
    "\n",
    "runsRemoved=0\n",
    "\n",
    "for param in allParams:\n",
    "    #rk does not have a 'iter_per_step' parameter\n",
    "    if param==-1:\n",
    "        filteredRunsPerParam[param]= [ run for run in allRuns if run['experiment']=='rk' ]\n",
    "    else:\n",
    "        filteredRunsPerParam[param]= \\\n",
    "        [ run for run in allRuns \\\n",
    "           if 'iters_per_step' in run['parameters'] \\\n",
    "           and run['parameters']['iters_per_step']==param ] #for this param value\n",
    "\n",
    "#keep only runs from the evaluation test set with the correct tunned parameter\n",
    "#evaluation set runs are the ones with the tunned parameter + all for rk\n",
    "\n",
    "#only instances in the evaluation set\n",
    "#for the runs of the tunnedParameter, filter out the runs of the tuning set\n",
    "evalRuns = [ \n",
    "    run for run in filteredRunsPerParam[tunedParam]  if 'tuning' not in run['set'] ] \\\n",
    "    + [run for run in filteredRunsPerParam[-1] if 'tuning' not in run['set'] \n",
    "]\n",
    "\n",
    "print(\"Number of runs in the evaluation set are\", len(evalRuns) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tunning set runs are only runs of kadabra\n",
    "\n",
    "#tunning runs are separated by the parameter value\n",
    "tuningRuns = dict()\n",
    "tuningSize = 0\n",
    "for x in allParams:\n",
    "    if x!=-1: #-1 signifies that runs for RK\n",
    "        if x==tunedParam:\n",
    "            #from the runs for the tunned parameter, get only the ones of the tunning set       \n",
    "            tuningRuns[x] = [ run for run in filteredRunsPerParam[x] if 'tuning' in run['set'] ]\n",
    "        else:\n",
    "            tuningRuns[x] = filteredRunsPerParam[x]                 \n",
    "\n",
    "        print( x, len(tuningRuns[x]), set([ x['instance'] for x in tuningRuns[x]]) )\n",
    "        tuningSize += len(tuningRuns[x])\n",
    "\n",
    "print( \"Number of runs in the tunning set are\", tuningSize )\n",
    "\n",
    "tuningInstSize = len( set([ x['instance'] for x in tuningRuns[1]]) )\n",
    "print(\"size of tuning set is \" + str(tuningInstSize) )\n",
    "#for next assertion: 3 is the repeated runs for the tunning set, 5 is the repeated runs for the \n",
    "#evaluation set, |allParams|-2 because we exclude the tunned valued (and -1 for rk)\n",
    "\n",
    "# in general: tunnigSetSize = |tunningSet|*( repeatForTunSet*(|param|-1) + repeatsForEvalSet )\n",
    "\n",
    "assert( tuningSize == ( tuningInstSize*(3*(len(allParams)-2)+3)) )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( \"Number of kadabra runs in the evaluation set:\", len([run['experiment'] for run in evalRuns if run['experiment']=='kadabra']) )\n",
    "print( \"Number of rk runs in the evaluation set:\", len([run['experiment'] for run in evalRuns if run['experiment']=='rk']) )\n",
    "assert( -1 not in tuningRuns ) # only runs of kadabra\n",
    "#same check as above, TODO: remove?\n",
    "for tRun in tuningRuns:\n",
    "    for t in tuningRuns[tRun]:\n",
    "        assert( t['experiment']=='kadabra')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sort lexicographicaly \n",
    "by tool name and then by number of nodes. \n",
    "This sorting also by size is important to ensure that ratios will be sorted according to instance size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allRuns.sort( key=itemgetter( 'experiment', 'n') )\n",
    "evalRuns.sort( key=itemgetter( 'experiment', 'n') )\n",
    "for tRun in tuningRuns:\n",
    "    tuningRuns[tRun].sort( key=itemgetter( 'experiment', 'n') )\n",
    "\n",
    "print(evalRuns[0])\n",
    "print(evalRuns[51])\n",
    "print(evalRuns[133])\n",
    "print(tuningRuns[100][0])\n",
    "#print(tunningRuns[1][0]['parameters'])\n",
    "#print(tunningRuns[100][0]['parameters'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract running times ratios\n",
    "- We sort the allRuns list lexicographicaly, first by experiment and then by instance size.\n",
    "- So, when we extract the instances names, these are also sorted by size.\n",
    "- Similarly, the ratios below are also sorted by the instance size.\n",
    "- Because of the sorting by names, the ratios are 'tool1_run_time' / 'tool2_run_time', where 'tool1_name'<'tool2_name'. In this case, the ration is kadabra/rk (since 'k'<'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the instances names and sizes.\n",
    "Instances might not be needed. Sizes will be used for plotting.\n",
    "Size can be the number of nodes, edges or both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the instances names\n",
    "#since allRuns are sorted, instances should be sorted by size (=number of nodes)\n",
    "allInstances = list( dict.fromkeys([ r['instance'] for r in allRuns]) ) #remove duplicates\n",
    "numInstances = len(allInstances)\n",
    "abbrvInst = abbreviate( allInstances );\n",
    "\n",
    "evaluationInst =  list( dict.fromkeys([run['instance'] for run in evalRuns]) ) # if 'tuning' not in run['set']]))\n",
    "numEvalInst = len(evaluationInst)\n",
    "tuningInst = list( dict.fromkeys([run['instance'] for run in tuningRuns[1]]) ) # if 'tuning' in run['set']]))\n",
    "numTunInst = len(tuningInst)\n",
    "print( \">>> tuning set length\",numTunInst  , \"and it is:\\n\", tuningInst )\n",
    "print( \">>> evaluation set length\", numEvalInst, \"and it is:\\n\",evaluationInst )\n",
    "\n",
    "#the two set do NOT have an empty intersection: tunningSet<evaluationSet\n",
    "assert( set(evaluationInst).isdisjoint(set(tuningInst)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the sizes of all instances and remove duplicates due to several runs per instance\n",
    "sizesN = list( dict.fromkeys([ r['n'] for r in evalRuns]) )\n",
    "sizesM = list( dict.fromkeys([ r['m'] for r in evalRuns]) )\n",
    "print(sizesN)\n",
    "sizesNM = [ n*m for n,m in zip(sizesN,sizesM)]\n",
    "sizesNplusM = [n+m for n,m in zip(sizesN,sizesM)]\n",
    "\n",
    "# make sure sizes are sorted and of equal size\n",
    "assert( sorted(sizesN)==sizesN)\n",
    "assert( len(sizesN)==len(sizesM) )\n",
    "assert( len(sizesN)==numEvalInst )\n",
    "\n",
    "print(\">>> all evaluation instances set length\", len(evaluationInst) , \"and they are:\\n\", [ (inst,n) for inst,n in zip(evaluationInst,sizesN)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algos = set([run.experiment.name for run in cfg.discover_all_runs()])\n",
    "assert( len(algos)==2 )\n",
    "print(\"Possible experiment names are\", algos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the mean running times using the arithmentic mean \n",
    "to summarize all running times for (tool,instance) in one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in general\n",
    "#timeToolX = [ r['run_time'] for r in allRuns if r['experiment']=='toolX' ]\n",
    "# or using a dictionary:\n",
    "#time['toolX'] = [ r['run_time'] for r in allRuns if r['experiment']=='toolX' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a dictionary of lists to store all running times for every instance \n",
    "# indexed by the instance name: allTimesRK['instanceX'] is a list of all\n",
    "# the run times with RK for instanceX, len(allTimesRK['instanceX'])=number of repeats\n",
    "\n",
    "stopTime = 60*60*7 #7 hours\n",
    "\n",
    "#will be used later for the box and violin plots\n",
    "allTimesRK = dict()\n",
    "allTimesKad = dict()\n",
    "\n",
    "#for one tool and one instance there are several running times\n",
    "#extract a list per tool and instance, calculate the mean and store it in a list\n",
    "\n",
    "# the mean running times of all runs for every instance, used for speedup and scatter plot\n",
    "timeRKMean = list()\n",
    "timeKadMean = list()\n",
    "evalBothInst = list() #evaluation instances for both algorithms\n",
    "\n",
    "#TODO: arithmentic or geometric mean?\n",
    "for inst in evaluationInst:\n",
    "    timeRKInst = [ r['run_time'] for r in evalRuns if r['experiment']=='rk' and r['instance']==inst ]\n",
    "    allTimesRK[inst] = timeRKInst\n",
    "    #maybe we do not have some instances for RK\n",
    "    if len(timeRKInst)!=0:\n",
    "        timeRKMean.append( sum(timeRKInst)/len(timeRKInst))\n",
    "        evalBothInst.append(inst)\n",
    "    else:\n",
    "        #timeRKMean.append( stopTime ) #TODO: what to add here?\n",
    "        timeRKMean.append( -1 )\n",
    "    \n",
    "    timeKadInst = [ r['run_time'] for r in evalRuns if r['experiment']=='kadabra' and r['instance']==inst ]\n",
    "    allTimesKad[inst] = timeKadInst\n",
    "    timeKadMean.append( sum(timeKadInst)/len(timeKadInst))\n",
    "\n",
    "nBoth = len(evalBothInst)\n",
    "missingInst = numEvalInst - nBoth\n",
    "onlyKadInst = list( set(evaluationInst)-set(evalBothInst))\n",
    "\n",
    "assert( len(onlyKadInst)==missingInst )\n",
    "assert( len(timeRKMean)==numEvalInst )\n",
    "assert( len(timeRKMean)==len(timeKadMean) )\n",
    "\n",
    "if nBoth != numEvalInst:\n",
    "    print(\"### Warning, we do not have results for all algorithm and all instances\")\n",
    "    print(\"### we have\", nBoth, \"for both but all evaluation instances are\", numEvalInst)\n",
    "    print(\"### instances only for kadabra:\", onlyKadInst)\n",
    "    #evaluationInst = evalBothInst.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WARNING\n",
    "we do not have data for the last 2 instances for RK. The following script will work if the instances missing\n",
    "are the largest ones, i.e. the last ones (if sorted by size). If we have other instances missing, say \n",
    "the 5th instance out 10, probably the rest of the code will not work correctly and need adaptation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the times for the tunning set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the mean running time per parameter value\n",
    "tuningTimeKadMean = dict()\n",
    "    \n",
    "for tRun in tuningRuns:\n",
    "    #18 = 3 repeats for 6 tunning instances\n",
    "    #125 only for the tunned valued # update: this should not happen now\n",
    "    print( \"param value:\", tRun, \", number of runs:\", len(tuningRuns[tRun]) )\n",
    "    \n",
    "    tuningTimeKadMean[tRun] = list()\n",
    "    thisRunInst = [x['instance'] for x in tuningRuns[tRun] ] #just for checking\n",
    "    for inst in tuningInst:\n",
    "        assert( inst in thisRunInst )\n",
    "        timeKad = [x['run_time'] for x in tuningRuns[tRun] if x['instance']==inst ]\n",
    "        tuningTimeKadMean[tRun].append( sum(timeKad)/len(timeKad) )\n",
    "        \n",
    "    #this is equal to the evaluation set size\n",
    "    assert( len(tuningTimeKadMean[tRun])== len(tuningInst) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test print\n",
    "print(\"instance \" + allInstances[12])\n",
    "print(allTimesKad[allInstances[12]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Various Visualization Plots - Running Time\n",
    "\n",
    "\n",
    "## After creating the running time ratios, plot them\n",
    "\n",
    "In section 6.Visualizing Results, about paragraph 4, it is mentioned: \n",
    "\n",
    "\"The x-axis, in turn, is used for the most relevant parameter describing the instances (e. g., instance size\n",
    "for a scalability evaluation or a graph parameter for evaluating its influence on the solution quality).\"\n",
    "\n",
    "According to that, the x-axis should have instance sizes and not instance names.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create ratios of running time\n",
    "# rk/kadabra\n",
    "#ratios = [ tRk/tKad for tRk, tKad in zip(timeRKMean,timeKadMean)]\n",
    "\n",
    "ratios= list()\n",
    "\n",
    "for tRk, tKad in zip(timeRKMean,timeKadMean):\n",
    "    #we do not have data for some instance for RK\n",
    "    if tRk!=-1:\n",
    "        ratios.append( tRk/tKad  )\n",
    "    #print( tRk, tKad)\n",
    "        \n",
    "assert( len(ratios)==nBoth )\n",
    "        \n",
    "if nBoth != numEvalInst:\n",
    "    print(\"###Warning, we do not have results for all algorithm and all instances\")\n",
    "    print(\"we have\", nBoth, \"for both but all evaluation instances are\", numEvalInst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( np.argmin(ratios), np.argmax(ratios))\n",
    "\n",
    "geomMean = round( np.array(ratios).prod()**(1.0/len(ratios)), 1 )\n",
    "minSpeedUp = round( min(ratios), 1)\n",
    "maxSpeedUp = round(max(ratios), 1)\n",
    "minSpeedUpInst = evaluationInst[np.argmin(ratios)]\n",
    "maxSpeedUpInst = evaluationInst[np.argmax(ratios)]\n",
    "# add the abbreviated instance name\n",
    "minSpeedUpInst += \" (\"+abbreviate(evaluationInst)[np.argmin(ratios)]+\")\"\n",
    "maxSpeedUpInst += \" (\"+abbreviate(evaluationInst)[np.argmax(ratios)]+\")\"\n",
    "\n",
    "print( \"min=\", minSpeedUp, minSpeedUpInst )\n",
    "print( \"max=\", maxSpeedUp, maxSpeedUpInst )\n",
    "print( \"geometric mean = \", geomMean)\n",
    "\n",
    "#arithmMean = sum(ratios)/len(ratios)\n",
    "#print(arithmMean)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,7))\n",
    "plt.rcParams.update({'font.size': 25})\n",
    "\n",
    "#plt.title('Overall speedup of KADABRA over RK for every instance in the evaluation set')\n",
    "plt.ylabel(\"algorithmic speedup (log scale)\", fontsize=23 )\n",
    "#plt.xlabel(\"Evaluation set\")\n",
    "plt.xticks( [i+1 for i in range(nBoth)] , abbreviate(evaluationInst), rotation=40, fontsize=25 )\n",
    "\n",
    "plt.bar( [i+1 for i in range(nBoth)] ,ratios, width=0.4)\n",
    "plt.plot( [0, numEvalInst+1], [geomMean, geomMean], label='geometric mean=' + str(geomMean), linewidth=2.5, color='m' )\n",
    "plt.xlim(left=0, right=nBoth+0.5)\n",
    "\n",
    "ax.annotate(geomMean, xy=(0, geomMean), xytext=(0, geomMean*1.1), color='m' )\n",
    "\n",
    "ax.set_yscale('log', basey=2)\n",
    "plt.ylim(top=700)\n",
    "\n",
    "plt.legend(loc=2)\n",
    "\n",
    "#pdf has problems with minus sign in the axes\n",
    "plt.savefig('../images/speedupPlot.pdf')\n",
    "plt.savefig('../images/speedupPlot.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if nBoth != numEvalInst:\n",
    "    print(\"###Warning, we do not have results for all algorithm and all instances\")\n",
    "    print(\"we have\", nBoth, \"for both but all evaluation instances are\", numEvalInst)\n",
    "    \n",
    "# there are many instances with the same diameter\n",
    "diams = list() \n",
    "for i in range(nBoth):\n",
    "    inst = evaluationInst[i]\n",
    "    for run in evalRuns:\n",
    "        if run['instance']==inst:\n",
    "            diams.append( run['diam'] )\n",
    "            break\n",
    "\n",
    "#actually. there is no need to sort...\n",
    "diamRatios = [ x for _,x in sorted( zip(diams,ratios), key=lambda pair: pair[0]) ]\n",
    "\n",
    "print(len(diamRatios))\n",
    "print(len(diams))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,8))\n",
    "plt.rcParams.update({'font.size': 22})\n",
    "\n",
    "plt.title('Overall speedup of KADABRA over RK for every instance in the evaluation set')\n",
    "plt.ylabel(\"running time speedup\")\n",
    "plt.xlabel(\"instance diameter\")\n",
    "\n",
    "#plt.scatter( diams , timeKadMean, marker='o', s=50 )\n",
    "#plt.scatter( diams , timeRKMean, marker='o', s=50 )\n",
    "\n",
    "#plt.scatter( sorted(diams) , diamRatios, marker='o', s=50 )\n",
    "#plt.scatter( diams , ratios, marker='o', s=30 )\n",
    "\n",
    "\n",
    "kadPlt = plt.scatter( diams, timeKadMean[:-missingInst], s=150, label='kadabra' )\n",
    "#kadPlt2 = plt.scatter( sizesNplusM[-1:], timeKadMean[-1:], s=150, marker='s', label='kadabra only', color=u'#1f77b4')\n",
    "RKplt = plt.scatter( diams, timeRKMean[:-missingInst], s=150, marker='P', label='rk' )\n",
    "\n",
    "ax.set_yscale('log', basey=2)\n",
    "ax.set_xscale('log', basex=2)\n",
    "\n",
    "#plt.savefig('../images/speedupPlot.pdf')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 8))\n",
    "ax.set_title(\"all speedups\")\n",
    "plt.ylabel(\"running time speedup\")\n",
    "plt.xlabel(\"TODO: x-axis would be how many instances have this speed up?\")\n",
    "\n",
    "ax.violinplot(ratios)\n",
    "\n",
    "#maybe we could combibe it with something like this:\n",
    "plt.bar( [i/150+1 for i in range(nBoth)], sorted(ratios,reverse=True) ,width=0.004 )\n",
    "#plt.scatter( [i/150+1 for i in range(numEvalInst)], sorted(ratios,reverse=True) ,s=15 )\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axarr = plt.subplots(4, 3, figsize=(15, 15))\n",
    "fig.subplots_adjust(hspace = 1.5, wspace=1.5)\n",
    "\n",
    "axarr = axarr.ravel()\n",
    "plt.rcParams.update({'font.size': 10})\n",
    "plt.setp( axarr, xticks=[i+1 for i in range(numEvalInst)], xticklabels=abbreviate(tuningInst) )\n",
    "#get speedups for the tunning set, for different parameter values\n",
    "tunRatios = dict()\n",
    "tunRunTime = dict()\n",
    "pos = 0\n",
    "\n",
    "for v in tuningRuns:\n",
    "    # rk/kadabra\n",
    "    tunRatios[v] = [ tRk/tKad for tRk, tKad in zip(timeRKMean,tuningTimeKadMean[v])]\n",
    "    axarr[pos].set_title(\"speedups for param value=\" +str(v) )\n",
    "    tunRunTime[v] = [ tKad for tKad in tuningTimeKadMean[v]]\n",
    "    print(v, np.mean(tunRunTime[v]) ) #, tunRunTime[v] )\n",
    "    #axarr[pos].boxplot( tunRatios[v] )\n",
    "    #axarr[pos].violinplot( tunRatios[v] )\n",
    "    axarr[pos].bar( [i+1 for i in range(len(tuningInst))] ,tunRunTime[v], width=0.3)\n",
    "    pos += 1\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import cm\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16,8))\n",
    "plt.rcParams.update({'font.size': 18})\n",
    "\n",
    "#plt.xlim(right=6)\n",
    "print(tuningInst)\n",
    "\n",
    "tuningSetSizes = list( dict.fromkeys([x['n'] for x in tuningRuns[1]]) )\n",
    "print(tuningSetSizes)\n",
    "\n",
    "colors=cm.rainbow(np.linspace(0,1,len(allParams)-1))\n",
    "pos = 0\n",
    "\n",
    "for i in sorted(allParams):\n",
    "    if i==-1:\n",
    "        continue\n",
    "#    plt.plot( tuningSetSizes , tunRunTime[i], marker='v', markersize=10, label=i , color=colors[pos], linestyle='' )\n",
    "    pos+=1\n",
    "\n",
    "plt.plot( tuningSetSizes , tunRunTime[10], marker='s', markersize=12, label='c=10' , color=colors[1], linestyle='' )\n",
    "plt.plot( tuningSetSizes , tunRunTime[1000], marker='x', markersize=12, label='c=1000' , color=colors[2], linestyle='' )\n",
    "plt.plot( tuningSetSizes , tunRunTime[4375], marker='v', markersize=12, label='c=4375' , color=colors[9], linestyle='' )\n",
    "plt.plot( tuningSetSizes , tunRunTime[10000], marker='o', markersize=12, label='c=10000' , color=colors[6], linestyle='' )\n",
    "    \n",
    "ax.set_yscale('log', basey=2)\n",
    "ax.set_xscale('log', basex=2)\n",
    "\n",
    "#plt.xticks( [math.pow(2,math.log(s,2)) for s in tunningSetSizes], abbreviate(tunningInst_X) )\n",
    "plt.xticks( tuningSetSizes, abbreviate(tuningInst), rotation=40, fontsize=22  )\n",
    "    \n",
    "plt.legend(loc='best',  bbox_to_anchor=(1, 1))\n",
    "plt.savefig('../images/allTunParams.pdf')\n",
    "plt.savefig('../images/allTunParams.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1.inset_locator import zoomed_inset_axes\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "from mpl_toolkits.axes_grid1.inset_locator import mark_inset\n",
    "\n",
    "#get geometric mean of all tunning parameters\n",
    "fig, ax = plt.subplots(figsize=(11,4))\n",
    "plt.rcParams.update({'font.size': 18})\n",
    "plt.rcParams['axes.linewidth'] = 1 \n",
    "\n",
    "tunParamGeomMean = list()\n",
    "tunParamArithmMean = list()\n",
    "\n",
    "for i in sorted(allParams):\n",
    "    if i==-1:\n",
    "        continue\n",
    "    tunParamGeomMean.append( np.array(tunRunTime[i]).prod()**(1.0/len(tunRunTime[i])) )\n",
    "    tunParamArithmMean.append( np.mean(tunRunTime[i]) )\n",
    "    #print( i, np.mean(tunRunTime[i]),   np.array(tunRunTime[i]).prod()**(1.0/len(tunRunTime[i]))  )\n",
    "    \n",
    "#print( [ (i,tunnParamArithmMean[i]) for i in range(len(tunnParamArithmMean))] )\n",
    "    \n",
    "ax.plot( sorted(allParams)[1:], tunParamArithmMean, marker='^', markersize=10 )\n",
    "\n",
    "ax.xlim = 100\n",
    "\n",
    "#plt.title(\" Speedup for 3 repeated runs and for all the graphs in the tunning set\")\n",
    "plt.ylabel(\"running time (sec)\", fontsize=23)\n",
    "plt.xlabel(\"tuning parameter value\", fontsize=23)\n",
    "\n",
    "#the inside, smaller axes\n",
    "axins = inset_axes( ax,\n",
    "                    width=\"50%\", # width = 30% of parent_bbox\n",
    "                    height=\"50%\", # height : 1 inch\n",
    "                    loc=9,\n",
    "                  )\n",
    "axins.alph=0.\n",
    "axins.plot( sorted(allParams)[5:-1], tunParamArithmMean[4:-1], marker='^', markersize=10)\n",
    "axins.set_xscale('log', basex=2)\n",
    "#axins.set_yscale('log', basey=2)\n",
    "\n",
    "axins.annotate( \"best tuning value\\nfor $c=\"+str(tunedParam)+\"$\", fontsize=15, \\\n",
    "               xy=(tunedParam, tunParamArithmMean[7]+1),\\\n",
    "               xytext=(tunedParam-1000,minTuningTime+15),\\\n",
    "               arrowprops=dict(facecolor='black', shrink=0.05), va = \"top\", ha=\"center\" )\n",
    "\n",
    "for axis in ['top','bottom','left','right']:\n",
    "    axins.spines[axis].set_linewidth(1.5)\n",
    "    axins.spines[axis].set_edgecolor('0.1')\n",
    "    \n",
    "##finish with the inside axes\n",
    "\n",
    "# set the sub region of the original image\n",
    "x1, x2, y1, y2 = 1850, 9000, 160, 182\n",
    "axins.set_xlim(x1, x2)\n",
    "axins.set_ylim(y1, y2)\n",
    "\n",
    "# draw a bbox of the region of the inset axes in the parent axes and\n",
    "# connecting lines between the bbox and the inset axes area\n",
    "mark_inset(ax, axins, loc1=3, loc2=4, fc=\"0.9\", ec=\"0.3\", alpha=0.9 )\n",
    "\n",
    "ax.set_xscale('log', basex=2)\n",
    "#ax.set_yscale('log', basey=2)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('../images/tunParam.pdf')\n",
    "plt.savefig('../images/tunParam.png')\n",
    "plt.draw()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting time/size to find the time needed per node \n",
    "(if \"size\"=$n$ , alternatively, \"size\"=$n+m$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: keep or remove lines?\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,8))\n",
    "plt.title('Mean running times over number of nodes (time per vertex)')\n",
    "plt.xlabel(\"instance size: number of nodes\")\n",
    "plt.ylabel(\"$\\log_{2}$(running time/n)\")\n",
    "\n",
    "print( len(sizesN), timeRKMean )\n",
    "\n",
    "plt.plot( sizesN, [ t/s for s,t in zip(sizesN,timeKadMean)], marker='o', label='kadabra' )\n",
    "plt.plot( sizesN[:-missingInst], [ t/s for s,t in zip(sizesN,timeRKMean)][:-missingInst], marker='o', label='rk' )\n",
    "ax.set_yscale('log', basey=2)\n",
    "ax.set_xscale('log', basex=2)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize with n+m (supposed theoretical running time) for every instance.\n",
    "We can also normalize with the theoretical running time of the algorithm.\n",
    "In this case, the result should a near-constant function. \n",
    "(Could not find a better theoretical running time in the kadabra paper.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize with n*m for every instance\n",
    "#TODO: keep or remove lines?\n",
    "fig, ax = plt.subplots(figsize=(15,8))\n",
    "plt.title('RK time normalised with n+m')\n",
    "plt.xlabel(\"instance size: number of nodes\") # TODO: here, should it be n*m? (need also to re-sort by n+m)\n",
    "plt.ylabel(\"running time/(n+m)\")\n",
    "\n",
    "plt.plot( sizesN, [ t/s for s,t in zip(sizesNplusM,timeKadMean)], marker='o', label='kadabra' )\n",
    "plt.plot( sizesN[:-1], [ t/s for s,t in zip(sizesNplusM,timeRKMean)][:-1], marker='o', label='rk' )\n",
    "ax.set_yscale('log', basey=2)\n",
    "ax.set_xscale('log', basex=2)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize size and times\n",
    "by dividing with the maximum running time for each algorithm and the maximum size.\n",
    "\n",
    "This is **not** mentioned in the paper and probably **not** of great use since it can give the\n",
    "wrong impression that kadabra is slower. \n",
    "I (harry) think that it may be useful to see how the running time of kadabra does not \n",
    "follow the theoreticaly expected behavior; at least not as much as RK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,8)) \n",
    "plt.title(\"Normalized running times and instance sizes\")\n",
    "\n",
    "plt.plot( [t/max(timeKadMean) for t in timeKadMean], label='kadabra' )\n",
    "plt.plot( [t/max(timeRKMean) for t in timeRKMean] , label='rk')\n",
    "plt.plot( [s/max(sizesNM) for s in sizesNM], label='sizes' )\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scatter plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( diams )\n",
    "\n",
    "from matplotlib.legend_handler import HandlerNpoints\n",
    "\n",
    "plt.rcParams.update({'font.size': 25})\n",
    "fig, ax = plt.subplots(figsize=(15,8))\n",
    "#plt.title(\"Scatter plot of mean running times\")\n",
    "plt.ylabel(\"running time (seconds)\")\n",
    "plt.xlabel(\"instance size: $n+m$\")\n",
    "\n",
    "kadPlt = plt.scatter( sizesNplusM[:-missingInst], timeKadMean[:-missingInst], s=150, label='KADABRA' )\n",
    "kadPlt2 = plt.scatter( sizesNplusM[-missingInst:], timeKadMean[-missingInst:], s=150, marker='^', label='KADABRA only', color=u'#1f77b4')\n",
    "RKplt = plt.scatter( sizesNplusM[:], timeRKMean[:], s=150, marker='P', label='RK' )\n",
    "#kadPlt2, = plt.plot( sizesNplusM[-2:], timeKadMean[-2:] , marker='s', label='kad', color=u'#1f77b4', linestyle='' )\n",
    "\n",
    "ax.set_yscale('log', basey=2)\n",
    "ax.set_xscale('log', basex=2)\n",
    "\n",
    "#uncomment to add labels for the graphs; they are cluttered\n",
    "#plt.xticks( sorted(sizesNplusM), abbreviate( [x for _, x in sorted(zip(sizesNplusM,evaluationInst), key=lambda pair: pair[0])] ),\\\n",
    "#           rotation=90, fontsize=15  )\n",
    "\n",
    "plt.legend()\n",
    "#plt.legend([ (kadPlt, kadPlt2), RKplt], [\"Attr A\" , \"Attr A+B\"])\n",
    "#plt.legend(handler_map={kadPlt2: HandlerLine2D(numpoints=3)})\n",
    "#plt.legend(handler_map={kadPlt2: HandlerNpoints(marker_pad=0.3, numpoints=3)})\n",
    "\n",
    "plt.savefig('../images/scatterPlot.pdf')\n",
    "plt.savefig('../images/scatterPlot.png')\n",
    "plt.show()\n",
    "\n",
    "#print(sizesNplusM)\n",
    "#print(timeKadMean)\n",
    "#print(timeRKMean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Box and violin plots for multiple runs per instance\n",
    "Extract all running times into a list of lists. This can be plotted directly by boxplot.\n",
    "\n",
    "The difference of running times for one instance are so small that these plots do not help. \n",
    "Especially for smaller values or where we have big differences between instances.\n",
    "Maybe the number of repeatitions is small (max is 10).\n",
    "Should we advice something for this case? Use some log scale (this must be done when creating the lists below)?\n",
    "If we focus on one (or few) instances? For example, instance 19 seems to be a good candidate for both algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: this below could be usefull if used with the positions argument\n",
    "# where positions = [1,2,4,5,7,8,10,11,...]\n",
    "# see https://stackoverflow.com/questions/16592222/matplotlib-group-boxplots\n",
    "\n",
    "#coupling times for every instance\n",
    "allTimesInterleaved = list()    \n",
    "\n",
    "for inst in evaluationInst:\n",
    "    allTimesInterleaved.append( allTimesKad[inst] )\n",
    "    allTimesInterleaved.append( allTimesRK[inst] )\n",
    "    \n",
    "#fig, ax = plt.subplots(figsize=(15, 8))    \n",
    "#bp = ax.boxplot(\n",
    "#    allTimesInterleaved[20:]\n",
    "#    )\n",
    "#ax.set_yscale('log', basey=2)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allTimesKadList = [ allTimesKad[inst] for inst in evaluationInst ]\n",
    "#print(allInstances[20:])\n",
    "    \n",
    "allTimesRKList = [ allTimesRK[inst]  for inst in evaluationInst ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 18})\n",
    "#fig1, ax1 = plt.subplots(figsize=(15, 8))\n",
    "f, (ax, ax2) = plt.subplots(2, 1, sharex=True, figsize=(15, 8), gridspec_kw = {'height_ratios':[1, 4]})\n",
    "\n",
    "#ax2.set_title(\"Kadabra running times\")\n",
    "plt.ylabel(\"running time (seconds)\")\n",
    "\n",
    "#the box plot and different parameters that control it\n",
    "# plot some interesting instances, leave the big ones out\n",
    "ax.boxplot(\n",
    "    allTimesKadList[10:-2],\n",
    "    notch=False,\n",
    "    showmeans=True,\n",
    ")\n",
    "ax2.boxplot(\n",
    "    allTimesKadList[10:-2],\n",
    "    notch=False,\n",
    "    showmeans=True,\n",
    ")\n",
    "\n",
    "#limit the two subplots\n",
    "ax.set_ylim(1350, 1500)\n",
    "ax2.set_ylim(0, 11)\n",
    "\n",
    "# hide the spines between ax and ax2\n",
    "ax.spines['bottom'].set_visible(False)\n",
    "ax2.spines['top'].set_visible(False)\n",
    "ax.xaxis.tick_top()\n",
    "ax.tick_params(labeltop=False)  # don't put tick labels at the top\n",
    "ax2.xaxis.tick_bottom()\n",
    "\n",
    "d = .015  # how big to make the diagonal lines in axes coordinates\n",
    "# arguments to pass to plot, just so we don't keep repeating them\n",
    "\n",
    "kwargs = dict(transform=ax.transAxes, color='k', clip_on=False)\n",
    "ax.plot((-d, +d), (-(d+0.07), +d), **kwargs)        # top-left diagonal\n",
    "ax.plot((1 - d, 1 + d), (-(d+0.07), +d), **kwargs)  # top-right diagonal\n",
    "\n",
    "kwargs.update(transform=ax2.transAxes)  # switch to the bottom axes\n",
    "ax2.plot((-d, +d), (1 - d, 1 + d), **kwargs)  # bottom-left diagonal\n",
    "ax2.plot((1 - d, 1 + d), (1 - d, 1 + d), **kwargs)  # bottom-right diagonal\n",
    "\n",
    "#plt.xticks( [i+1 for i in range(len(allTimesKadList))] , abbreviate(evaluationInst), rotation=40, fontsize=20 )\n",
    "plt.xticks( [i+1 for i in range(len(allTimesKadList[10:-2]))] , abbreviate(evaluationInst[10:-2]), rotation=40, fontsize=25 )\n",
    "\n",
    "f.subplots_adjust(hspace=0.07)\n",
    "\n",
    "plt.savefig('../images/boxPlotKad.pdf')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 18})\n",
    "#to cut into 3 parts\n",
    "#f, (ax, ax2, ax3) = plt.subplots(3, 1, sharex=True, figsize=(15, 8), gridspec_kw = {'height_ratios':[1,2,3]})\n",
    "f, (ax, ax2) = plt.subplots(2, 1, sharex=True, figsize=(15, 8), gridspec_kw = {'height_ratios':[1,3]})\n",
    "\n",
    "plt.ylabel(\"running time (seconds)\")\n",
    "\n",
    "ax.violinplot( allTimesRKList[10:-2] , widths=0.5 )\n",
    "ax2.violinplot( allTimesRKList[10:-2] , widths=0.5 )\n",
    "#ax3.violinplot( allTimesRKList[10:] , widths=0.5 )\n",
    "\n",
    "#limit the two subplots\n",
    "ax.set_ylim(8480, 8800)\n",
    "ax2.set_ylim(50, 3050)\n",
    "#ax3.set_ylim(50, 900)\n",
    "\n",
    "# hide the spines between ax and ax2\n",
    "ax.spines['bottom'].set_visible(False)\n",
    "ax2.spines['top'].set_visible(False)\n",
    "ax.xaxis.tick_top()\n",
    "ax.tick_params(labeltop=False)  # don't put tick labels at the top\n",
    "ax2.xaxis.tick_bottom()\n",
    "\n",
    "d = .015  # how big to make the diagonal lines in axes coordinates\n",
    "# arguments to pass to plot, just so we don't keep repeating them\n",
    "\n",
    "kwargs = dict(transform=ax.transAxes, color='k', clip_on=False)\n",
    "ax.plot((-d, +d), (-(d+0.05), +d), **kwargs)        # top-left diagonal\n",
    "ax.plot((1 - d, 1 + d), (-(d+0.05), +d), **kwargs)  # top-right diagonal\n",
    "\n",
    "kwargs.update(transform=ax2.transAxes)  # switch to the bottom axes\n",
    "ax2.plot((-d, +d), (1 - d, 1 + d), **kwargs)  # bottom-left diagonal\n",
    "ax2.plot((1 - d, 1 + d), (1 - d, 1 + d), **kwargs)  # bottom-right diagonal\n",
    "f.subplots_adjust(hspace=0.07)\n",
    "\n",
    "plt.xticks( [(i+1) for i in range(len(allTimesKadList[10:-2]))] , abbreviate(evaluationInst[10:-2]), rotation=40, fontsize=25 )\n",
    "\n",
    "plt.savefig('../images/violinPlotRK.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate and plot the variance for repeated runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varianceKad = list()\n",
    "print(len(allTimesKadList))\n",
    "#for tK, tR in zip(allTimesKadList, allTimesRKList):\n",
    "    #print( np.var(tK), np.var(tR) )\n",
    "\n",
    "varianceKad = [ np.var(t)/np.mean(t) for t in allTimesKadList]\n",
    "varianceRK = [ np.var(t)/np.mean(t) for t in allTimesRKList[:-1]]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,8))\n",
    "\n",
    "plt.plot( sizesN, varianceKad , marker='o', label='kadabra') #, linestyle='')\n",
    "plt.plot( sizesN[:-1], varianceRK , marker='o', label='rk') #, linestyle='' )\n",
    "\n",
    "#ax.bar( [5*i+1 for i in range(len(sizesN))] , varianceKad, width=0.8, label='kadabra')\n",
    "#ax.bar( [5*i for i in range(len(sizesN)-2)] , varianceRK, width=0.8, label='rk')\n",
    "\n",
    "plt.xticks( [ 5*i for i in range(numEvalInst)] , abbreviate(evaluationInst), rotation=40, fontsize=22 )\n",
    "\n",
    "ax.set_yscale('log', basey=2)\n",
    "ax.set_xscale('log', basex=2)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Violin plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig3, ax3 = plt.subplots(figsize=(15, 8))\n",
    "ax3.set_title(\"Kadabra running times for every instance\")\n",
    "#plt.ylabel(\"running time\")\n",
    "\n",
    "#ax3.boxplot( [ tK-np.mean(tK) for tK in allTimesKadList] )\n",
    "ax3.boxplot( allTimesKadList , showmeans=True)\n",
    "\n",
    "plt.xticks( [i+1 for i in range(numEvalInst)] , abbreviate(evaluationInst), rotation=40, fontsize=20 )\n",
    "#plt.xticks( [i+1 for i in range(len(allTimesKadList[10:]))] , abbreviate(evaluationInst[10:]), rotation=40, fontsize=20 )\n",
    "\n",
    "ax3.set_yscale('log', basey=2)\n",
    "#plt.ylim(bottom=0.0000001)\n",
    "\n",
    "plt.savefig('../images/violinPlotKad.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 8))\n",
    "ax.set_title(\"RK running times for every instance\")\n",
    "#plt.ylabel(\"running time\")\n",
    "\n",
    "ax.boxplot( allTimesRKList )\n",
    "\n",
    "plt.xticks( [i+1 for i in range(numEvalInst)] , abbreviate(evaluationInst), rotation=40, fontsize=20 )\n",
    "#plt.xticks( [i+1 for i in range(len(allTimesKadList[10:]))] , abbreviate(evaluationInst[10:]), rotation=40, fontsize=20 )\n",
    "\n",
    "ax.set_yscale('log', basey=2)\n",
    "#plt.ylim(bottom=0.0000001)\n",
    "\n",
    "#plt.savefig('../images/violinPlotKad.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig3, ax3 = plt.subplots(figsize=(15, 8))\n",
    "plt.rcParams.update({'font.size': 25})\n",
    "plt.ylabel(\"relative deviation in running time\")\n",
    "\n",
    "ax3.boxplot( [ (tRK-np.mean(tRK))/np.mean(tRK) for tRK in allTimesRKList[:-missingInst]] )\n",
    "\n",
    "yTicks = [i/100 for i in range(-4, 5, 1)]\n",
    "print(yTicks)\n",
    "\n",
    "# [:-missingInst] because RK is missing data for a road network\n",
    "plt.xticks( [i+1 for i in range(numEvalInst+1)][:-missingInst] , abbreviate(evaluationInst[:-missingInst]), rotation=40, fontsize=25 )\n",
    "plt.yticks( yTicks , [ str(y*100)+'%' for y in yTicks] , fontsize=25 )\n",
    "\n",
    "plt.savefig('../images/relRunTimeDev_RK.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 8))\n",
    "plt.rcParams.update({'font.size': 25})\n",
    "#ax.set_title(\"Kadabra\")\n",
    "plt.ylabel(\"relative deviation in running time\")\n",
    "relDevKad  = [ (tKad-np.mean(tKad))/np.mean(tKad) for tKad in allTimesKadList]\n",
    "\n",
    "ax.violinplot( relDevKad, showmeans=True )\n",
    "#ax3.violinplot( allTimesRKList[:-2] )\n",
    "\n",
    "yTicks = [i/100 for i in range(-16, 17, 4)]\n",
    "\n",
    "assert( numEvalInst==len(evaluationInst))\n",
    "\n",
    "plt.xticks( [i+1 for i in range(numEvalInst)] , abbreviate(evaluationInst), rotation=40, fontsize=25 )\n",
    "plt.yticks( yTicks , [ str(y*100)+'%' for y in yTicks] , fontsize=25 )\n",
    "\n",
    "#ax3.set_yscale('log', basey=2)\n",
    "plt.xlim(left=0.5, right=24.5)\n",
    "\n",
    "plt.savefig('../images/relRunTimeDev_Kad.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 8))\n",
    "#ax1.set_title(\"Kadabra running times for every instance\")\n",
    "plt.ylabel(\"running time\")\n",
    "\n",
    "numPlots = len(allTimesKadList[10:])\n",
    "\n",
    "#the box plot and different parameters that control it\n",
    "bp = ax.boxplot(\n",
    "    allTimesKadList[10:],\n",
    "    notch=False,\n",
    "    showmeans=True,\n",
    "    boxprops = dict(linewidth=1, markersize=30),\n",
    "    positions= [2*i for i in range(numPlots)]\n",
    ")\n",
    "ax.violinplot( \n",
    "    allTimesKadList[10:],\n",
    "    positions= [2*i+1 for i in range(numPlots)]\n",
    ")\n",
    "plt.xlim(right=25.5)\n",
    "#plt.xticks( [i+1 for i in range(allTimesKadList)] , abbreviate(evaluationInst), rotation=40, fontsize=20 )\n",
    "plt.xticks( [2*i+0.5 for i in range(numPlots)] , abbreviate(evaluationInst[10:]), rotation=40, fontsize=20 )\n",
    "\n",
    "#plt.setp(bp['boxes'], color='r')    \n",
    "plt.savefig('../images/boxViolinKad.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig, ax = plt.subplots( figsize=(15, 8))\n",
    "fig, ax = plt.subplots(figsize=(15, 8))\n",
    "\n",
    "ax = plt.subplot(1,2,1)\n",
    "\n",
    "plt.rcParams.update({'font.size': 26})\n",
    "\n",
    "meanpointprops = dict(markersize=15, linestyle='-.', linewidth=3)\n",
    "medianlineprops = dict(linestyle='-', linewidth=3)\n",
    "\n",
    "bp = ax.boxplot(\n",
    "    allTimesKadList[-3],\n",
    "    showmeans=True,\n",
    "    #notch=True,\n",
    "    boxprops = dict(linewidth=3, markersize=30),\n",
    "    meanprops=meanpointprops,\n",
    "    medianprops = medianlineprops,\n",
    "    flierprops=dict(markersize=20),\n",
    ")\n",
    "\n",
    "ax = plt.subplot(1,2,2)\n",
    "\n",
    "parts = ax.violinplot( allTimesRKList[-3], positions=[1.5],\n",
    "              showmedians=True,\n",
    "             )\n",
    "#for pc in parts['bodies']:\n",
    "    #pc.set_facecolor('#D43F3A')\n",
    "    #pc.set_edgecolor('black'),\n",
    "    #pc.set_linewidths(15)\n",
    "    #pc.set_zorder(4)\n",
    "    #pc.set_alpha(0.5)\n",
    "    \n",
    "plt.xlim( left=0.8, right = 1.8)\n",
    "plt.xticks( [1.25],  evaluationInst[-3:-2] )\n",
    "#ax.set_yscale('log', basey=2)\n",
    "\n",
    "plt.savefig('../images/boxViolion.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as mpatches\n",
    "\n",
    "fig4, ax4 = plt.subplots(figsize=(15, 8))\n",
    "#ax4.set_title(\"RK running times for every instance\")\n",
    "plt.ylabel(\"running time\")\n",
    "\n",
    "kad_patch = mpatches.Patch(color=u'#1f77b4', label='kadabra')\n",
    "rk_patch = mpatches.Patch(color=u'#ff7f0e', label='rk')\n",
    "#print( allTimesRKList[10:] )\n",
    "ax4.violinplot( allTimesKadList[10:] ) #, positions=sizesNplusM[10:], widths=100)\n",
    "ax4.violinplot( allTimesRKList[10:-missingInst] ) #, positions=sizesNplusM[10:-2], widths=100)\n",
    "\n",
    "ax4.set_yscale('log', basey=2)\n",
    "#ax4.set_xscale('log', basex=2)\n",
    "#plt.xticks( [i+1 for i in range(numEvalInst)] , abbreviate(evaluationInst), rotation=40, fontsize=20 )\n",
    "plt.legend(loc=5, handles=[kad_patch, rk_patch])\n",
    "\n",
    "plt.savefig('../images/violinScatter.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Errorbar plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,8))\n",
    "    \n",
    "lowErrorKad = []\n",
    "upErrorKad = []\n",
    "\n",
    "lowErrorRK = []\n",
    "upErrorRK = []\n",
    "\n",
    "log_base = 2\n",
    "\n",
    "for i in range(numEvalInst):\n",
    "    inst = evaluationInst[i]\n",
    "    \n",
    "    instKadRun = [run['run_time'] for run in evalRuns if (run['experiment']=='kadabra' and run['instance']==inst) ]\n",
    "    #print(inst, instKadRun)\n",
    "    lowErrorKad.append( timeKadMean[i]-min(instKadRun) )\n",
    "    upErrorKad.append( max(instKadRun)-timeKadMean[i] )\n",
    "    \n",
    "    instRKRun = [run['run_time'] for run in evalRuns if (run['experiment']=='rk' and run['instance']==inst) ]\n",
    "    if len(instRKRun)==0:\n",
    "        lowErrorRK.append( timeRKMean[i] )\n",
    "        upErrorRK.append( timeRKMean[i] )\n",
    "        continue\n",
    "    lowErrorRK.append( timeRKMean[i]-min(instRKRun) )\n",
    "    upErrorRK.append( max(instRKRun)-timeRKMean[i] )\n",
    "\n",
    "#print(timeKadMean)\n",
    "fig, ax = plt.subplots(figsize=(15,8))\n",
    "\n",
    "plt.ylabel(\"mean running time\")\n",
    "plt.xlabel(\"number of nodes\")\n",
    "\n",
    "plt.errorbar( [i+1 for i in range(numEvalInst) ][10:], timeKadMean[10:], yerr=[lowErrorKad[10:], upErrorKad[10:]], elinewidth=4, linestyle='--',label='kadabra' )\n",
    "plt.errorbar( [i+1 for i in range(numEvalInst) ][10:-1], timeRKMean[10:-1], yerr=[lowErrorRK[10:-1], upErrorRK[10:-1]], elinewidth=4, linestyle='--', label='rk')\n",
    "\n",
    "ax.set_yscale('log', basey=2)\n",
    "ax.set_xscale('log', basex=2)\n",
    "\n",
    "#plt.errorbar(sizesN, timeKadMean, yerr=[lowErrorKad, upErrorKad], label='kadabra' )\n",
    "#plt.errorbar(sizesN, timeRKMean, yerr=[lowErrorRK, upErrorRK], label='rk')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Various Visualization Plots - Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#taken from Moritz's Analysis notebook\n",
    "\n",
    "# Load exact betweenness values for each instance\n",
    "exactValues = dict()\n",
    "exactValueDir = \"../experiments/output/brandes/\"\n",
    "for instance in cfg.all_instances():\n",
    "    with open(os.path.join(exactValueDir, instance.filename + '.out-full'), 'r') as f:\n",
    "        exactValues[instance.filename] = np.loadtxt(f)\n",
    "        \n",
    "    #brandes scores are not relative: normalize witn (n-1)(n-2)\n",
    "        \n",
    "    if len([ x['n'] for x in evalRuns if x['instance']==instance.shortname] )==0:\n",
    "        print(\"No data for\", instance.shortname, \", skipping\")\n",
    "        continue\n",
    "    instSize = [ x['n'] for x in evalRuns if x['instance']==instance.shortname][0]\n",
    "    \n",
    "    exactValues[instance.filename] = [ x/((instSize-1)*(instSize-2)) for x in exactValues[instance.filename] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the full betweenness scores for each instance\n",
    "\n",
    "# - fullResults[inst][algo] is a list with size = the number of repeated runs\n",
    "# - fullResults[inst][algo][i] is a list of size = size(instance) and contains\n",
    "#     the betweenness value for every node of inst\n",
    "\n",
    "fullResults = dict()\n",
    "for run in cfg.discover_all_runs():\n",
    "    instance = run.instance.filename\n",
    "    algo = run.experiment.name\n",
    "    \n",
    "    if instance not in [x['instance'] for x in evalRuns]:\n",
    "        print( \"Instance\", instance,\"not in evaluation set, skipping\")\n",
    "        continue\n",
    "        \n",
    "    if not instance in fullResults:\n",
    "        fullResults[instance] = dict()\n",
    "    if not algo in fullResults[instance]:\n",
    "        fullResults[instance][algo] = list()\n",
    "        \n",
    "    fullFilename = run.output_file_path('out')+\"-full\"\n",
    "    try:\n",
    "        #print(\"opening\", fullFilename, \"for\", instance)\n",
    "        with open(fullFilename, 'r') as f:\n",
    "            fullResults[instance][algo].append(np.loadtxt(f))\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    #assert sizes agree\n",
    "    if len(fullResults[instance][algo])==0:\n",
    "        print(fullFilename)\n",
    "        print(\"No data for\", instance, \"and algo\", algo)\n",
    "        continue\n",
    "    instSize1 = len(fullResults[instance][algo][0])\n",
    "    instSize2 = [ x['n'] for x in evalRuns if x['instance']==instance][0]\n",
    "    \n",
    "    #TODO: this gives a warning (or shoulb be error?). Ignore for now\n",
    "    if instSize1!=instSize2:\n",
    "        print(\"WARNING: Sizes for instace \" + instance + \" do not agree. One is \"\\\n",
    "              + str(instSize1) + \" and the other is \" + str(instSize2) )\n",
    "        #break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test print\n",
    "print('number of repeated runs for rk and ca-cit-HepTh', len(fullResults['ca-cit-HepTh']['rk']))\n",
    "print('number of nodes of ca-cit-HepTh', len(fullResults['ca-cit-HepTh']['rk'][0]))\n",
    "\n",
    "#number of runs, should be 10 but some have just one\n",
    "print( 'number of repeated runs for kadabra and ca-AstroPh', len(fullResults['ca-AstroPh']['kadabra']) )\n",
    "print('number of repeated runs for rk and opsahl-powergrid', len(fullResults['wiki-Vote']['rk']) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algos = set([run.experiment.name for run in cfg.discover_all_runs()])\n",
    "assert( len(algos)==2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avgErrors = dict()\n",
    "distance = dict()\n",
    "allRunDiffs = dict()\n",
    "\n",
    "for algo in algos:\n",
    "    avgErrors[algo] = []\n",
    "    distance[algo] = []\n",
    "    allRunDiffs[algo] = dict()\n",
    "    \n",
    "    #WARNING: -1 because we do not have data with rk and one road network\n",
    "    for instance in evaluationInst[:-missingInst]: #cfg.all_instances():\n",
    "        allRunDiffs[algo][instance] = []\n",
    "        \n",
    "        size = len(exactValues[instance])\n",
    "        \n",
    "        #here, the [0] means that we only take the results from the first run\n",
    "        #diff = [abs(exactValues[instance.filename][i] - fullResults[instance.filename][algo][0][i]) for i in range(size)]\n",
    "        \n",
    "        # number of repeated runs for this instance\n",
    "        numRuns = len(fullResults[instance][algo])\n",
    "        print(\"number of runs for instance \" + instance + \" and algo \"+ algo + \" is \" + str(numRuns) )\n",
    "        for j in range(numRuns):\n",
    "            diffN = [abs(exactValues[instance][i] - fullResults[instance][algo][j][i]) for i in range(size)]\n",
    "            diff = sum(diffN)/size\n",
    "            #print(diff)\n",
    "            allRunDiffs[algo][instance].append(diff)\n",
    "        \n",
    "        betw1 = exactValues[instance]\n",
    "        betw2 = fullResults[instance][algo][0]\n",
    "        dist = np.linalg.norm( betw1-betw2 )\n",
    "        #print( len(exactValues[instance.filename]))\n",
    "        \n",
    "        distance[algo].append(dist)\n",
    "                \n",
    "        # average of differences for all repeated runs\n",
    "        avgErrors[algo].append( sum(allRunDiffs[algo][instance])/numRuns )\n",
    "        #avgErrors[algo].append(sum(diff)/size)\n",
    "\n",
    "assert( len(avgErrors['rk'])==nBoth )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test prints\n",
    "print( 'number of algos:' , len(allRunDiffs) )  # number of algos\n",
    "print( 'number of instances:' , len(allRunDiffs['rk']) )  # number of instances\n",
    "print( 'number of repeated runs for rk and \\'openflights\\':',len(allRunDiffs['rk']['openflights']) ) # number of repeated runs for instance and algo\n",
    "print('4th instance is', evaluationInst[4])\n",
    "#print( 'all solution differences for rk and ' + instances[11] +': ', allRunDiffs['rk-1t'][instances[11]] ) # all differences for the repeated runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,8))\n",
    "\n",
    "plt.title('Dirrences from exact betweenness values')\n",
    "meanBtwKad = [sum(allRunDiffs['kadabra'][inst])/len(allRunDiffs['kadabra'][inst]) for inst in evaluationInst[:-missingInst]] \n",
    "meanBtwRK = [sum(allRunDiffs['rk'][inst])/len(allRunDiffs['rk'][inst]) for inst in evaluationInst[:-missingInst]] \n",
    "\n",
    "#plt.scatter( [i for i in range(numEvalInst)][:-1], meanBtwKad , s=100, label='kadabra' )\n",
    "#plt.scatter( [i for i in range(numEvalInst)][:-1], meanBtwRK , s=100, marker='P', label='rk' )\n",
    "\n",
    "ax.bar( [5*i+1 for i in range(numEvalInst)][:-missingInst] , meanBtwKad, width=0.8, label='kadabra')\n",
    "ax.bar( [5*i for i in range(numEvalInst)][:-missingInst] , meanBtwRK, width=0.8, label='rk')\n",
    "\n",
    "plt.xticks( [5*i for i in range(numEvalInst)][:-missingInst], abbreviate(evaluationInst), rotation=40, fontsize=20 )\n",
    "plt.ylim(top=0.0015, bottom=-0.00001)\n",
    "\n",
    "#ylim for log scale\n",
    "#plt.ylim(top=0.015, bottom=0.00000002)\n",
    "#ax.set_yscale('log', basey=2)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,8))\n",
    "plt.rcParams.update({'font.size': 22})\n",
    "\n",
    "plt.title('Quality: average difference of betweenness compared to ground truth')\n",
    "plt.xlabel(\"instance size: number of nodes\")\n",
    "plt.plot( [i for i in range(numEvalInst-missingInst)], avgErrors['kadabra'], marker='o', label='kadabra' )\n",
    "plt.plot( [i for i in range(numEvalInst-missingInst)], avgErrors['rk'], marker='o', label='rk')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "plt.rcParams.update({'font.size': 22})\n",
    "\n",
    "plt.title('Quality: euclidean distance of betweenness vectors from ground truth')\n",
    "plt.xlabel(\"instance size: number of nodes\")\n",
    "plt.plot( sizesN[:-missingInst], distance['kadabra'], marker='o', label='kadabra' )\n",
    "plt.plot( sizesN[:-missingInst], distance['rk'], marker='o', label='rk')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: we do not want to print the betweenness value but the difference from the ground truth\n",
    "\n",
    "allScoresKadList = list()\n",
    "allDiffsKad = list()\n",
    "\n",
    "for inst in evalBothInst:\n",
    "    allScoresKadList.append( fullResults[inst]['kadabra'] ) \n",
    "    allDiffsKad.append( allRunDiffs['rk'][inst] )\n",
    "    \n",
    "print( len(allDiffsKad) ) \n",
    "print( allDiffsKad[5])\n",
    "#allTimesRKList = list()\n",
    "#for i in range(numInstances):\n",
    "    #if timeRKMean[i] > 2500:\n",
    "#    allTimesRKList.append( allTimesRK[instances[i]] ) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2, ax2 = plt.subplots(figsize=(15, 8))\n",
    "ax2.set_title(\"betweenness scores\")\n",
    "\n",
    "ax2.boxplot(\n",
    "    #allScoresKadList[0],\n",
    "    allDiffsKad[0],\n",
    "    notch=True,\n",
    "    sym='o',\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig3, ax3 = plt.subplots(figsize=(15, 8))\n",
    "ax3.set_title(\"betweenness scores\")\n",
    "\n",
    "#plt.scatter( [i for i in range(numInstances)], [exactValues[inst] for inst in instances], s=50, label='exact' )\n",
    "\n",
    "ax3.boxplot(\n",
    "    [exactValues[inst] for inst in evaluationInst][0],\n",
    "    #allDiffsKad[0],\n",
    "    notch=False,\n",
    "    #sym='o',\n",
    ")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to write macros for numerical values that are used in main.tex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_macro(file, name, value):\n",
    "    file.write( '\\\\newcommand{\\\\' + name + '}{' + value + '}\\n' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the file to store the macros\n",
    "macrosF = open('../numerical_macros.tex', 'w')\n",
    "\n",
    "make_macro( macrosF, \"tunParamValue\", str(tunedParam) )\n",
    "make_macro( macrosF, \"minTunParamTime\", str(minTuningTime) )\n",
    "\n",
    "make_macro( macrosF, \"speedUpGeomMean\", str(geomMean) )\n",
    "make_macro( macrosF, \"speedUpMin\", str(minSpeedUp) )\n",
    "make_macro( macrosF, \"minSpeedUpInst\", minSpeedUpInst )\n",
    "make_macro( macrosF, \"speedUpMax\", str(maxSpeedUp) )\n",
    "make_macro( macrosF, \"maxSpeedUpInst\", maxSpeedUpInst )\n",
    "\n",
    "strOnlyKad = \"\"\n",
    "for s in onlyKadInst:\n",
    "    print(s)\n",
    "    strOnlyKad += s + \" (\"+abbreviate([s])[0]+ \"), \"\n",
    "strOnlyKad = strOnlyKad[:-2] #remove the comma and space\n",
    "\n",
    "make_macro( macrosF, \"onlyKadInstances\", strOnlyKad )\n",
    "\n",
    "macrosF.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
