{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import aextl\n",
    "import numpy\n",
    "import os\n",
    "import yaml\n",
    "import math\n",
    "\n",
    "import scipy\n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-darkgrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experimentDir = \"../experiments/\"\n",
    "exactValueDir = \"../experiments/output/brandes/\"\n",
    "#suffix = '-1t'\n",
    "suffix = ''\n",
    "selectedVariant = 'ips4375'\n",
    "selectedInstSet = 'evaluation'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in Experiment Manifest File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = aextl.config_for_dir(experimentDir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in Experimental Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load exact betweenness values for each instance\n",
    "exactValues = dict()\n",
    "\n",
    "for instance in cfg.all_instances():\n",
    "    try:\n",
    "        with open(os.path.join(exactValueDir, instance.filename + '.out-full'), 'r') as f:\n",
    "            exactValues[instance.filename] = numpy.loadtxt(f)\n",
    "        size = len(exactValues[instance.filename])\n",
    "        normalizingConstant = (size-1)*(size-2)\n",
    "        for i in range(size):\n",
    "            exactValues[instance.filename][i] /= normalizingConstant\n",
    "    except FileNotFoundError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluationInstances = set([instance for instance in cfg.all_instances() if selectedInstSet in instance.instsets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the full betweenness scores for each instance\n",
    "fullResults = dict()\n",
    "for instance in cfg.all_instances():\n",
    "    fullResults[instance.filename] = dict()\n",
    "\n",
    "for run in cfg.discover_all_runs():\n",
    "    algo = run.experiment.name\n",
    "    variant = None if len(run.experiment.variation) == 0 else run.experiment.variation[0].name\n",
    "    if algo != ('kadabra'+suffix) or variant == selectedVariant:\n",
    "        instance = run.instance.filename\n",
    "        \n",
    "        if not algo in fullResults[instance]:\n",
    "            fullResults[instance][algo] = list()\n",
    "\n",
    "        fullFilename = run.output_file_path('out')+\"-full\"\n",
    "        try:\n",
    "            with open(fullFilename, 'r') as f:\n",
    "                fullResults[instance][algo].append(numpy.loadtxt(f))\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all running times into a single list of dictionaries\n",
    "def load_file(run, f):\n",
    "    data = yaml.load(f)\n",
    "    return {\n",
    "        'experiment': run.experiment.name,\n",
    "        'variant': run.experiment.variation,\n",
    "        'instance': run.instance.filename,\n",
    "        'run_time': data['run_time'],\n",
    "        'parameters' : data['parameters']\n",
    "    }\n",
    "\n",
    "times = cfg.collect_successful_results(load_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for instance in cfg.all_instances():\n",
    "    print(instance.filename, instance.instsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completedInstances = dict()\n",
    "\n",
    "for instance in cfg.all_instances():\n",
    "    for run in times:\n",
    "        algo = run['experiment']\n",
    "        if not algo in completedInstances:\n",
    "            completedInstances[algo] = set()\n",
    "        \n",
    "        if algo != 'kadabra'+suffix or (len(run['variant']) > 0 and run['variant'][0].name == selectedVariant):\n",
    "            if run['instance'] == instance.filename:\n",
    "                completedInstances[algo].add(instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(completedInstances['rk'+suffix])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Significance Test Comparing Approximation Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get average errors for each algorithm and instance\n",
    "avgErrors = dict()\n",
    "\n",
    "# We compare the results only on those instances from the evaluation set on which both algorithms finished\n",
    "compareInstances = evaluationInstances & completedInstances['rk'+suffix] & completedInstances['kadabra'+suffix]\n",
    "# Sort the instances by vertex count\n",
    "sortedInstances = sorted(compareInstances, key=lambda x : len(exactValues[x.filename]) )\n",
    "\n",
    "for instance in compareInstances:\n",
    "    instanceName = instance.filename\n",
    "    avgErrors[instanceName] = dict()\n",
    "    size = len(exactValues[instanceName])\n",
    "    for algo in fullResults[instanceName].keys():\n",
    "        if len(fullResults[instanceName][algo]) == 0:\n",
    "            print(\"No full results for \" + algo + \" on \" + instanceName)\n",
    "            continue\n",
    "        if len(fullResults[instanceName][algo][0]) != size:\n",
    "            print(\"Result for \" + algo + \" on \" + instanceName + \" has length \" \\\n",
    "              + str(len(fullResults[instanceName][algo][0])) + \" instead of \" + str(size))\n",
    "            continue\n",
    "\n",
    "        diff = [abs(exactValues[instanceName][i] - fullResults[instanceName][algo][0][i]) for i in range(size)]\n",
    "        avgErrors[instanceName][algo] = sum(diff)/size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare measured difference between average errors with the null hypothesis of no difference\n",
    "rkErrors = []\n",
    "kadabraErrors = []\n",
    "\n",
    "for instance in sortedInstances:\n",
    "        rkErrors.append(avgErrors[instance.filename]['rk'+suffix])\n",
    "        kadabraErrors.append(avgErrors[instance.filename]['kadabra'+suffix])\n",
    "\n",
    "scipy.stats.wilcoxon(rkErrors, kadabraErrors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[kadabraErrors[i]/rkErrors[i] for i in range(len(rkErrors))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([kadabraErrors[i]*10**5 for i in range(len(rkErrors))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Parameter Estimation for Scaling Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pymc3 as pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take logarithms of sizes and running times as preparation for scaling model\n",
    "algos = ['rk'+suffix, 'kadabra'+suffix]\n",
    "logTimes = dict()\n",
    "for algo in algos:\n",
    "    logTimes[algo] = dict()\n",
    "\n",
    "for run in times:\n",
    "    instance = run['instance']\n",
    "    runtime = run['run_time']\n",
    "    \n",
    "    if run['experiment'] == algos[1]:\n",
    "        if len(run['variant']) == 0 or not run['variant'][0].name == selectedVariant:\n",
    "            continue\n",
    "    \n",
    "    logTimes[run['experiment']][instance] = math.log(runtime)\n",
    "    \n",
    "logTimesRK = []\n",
    "logTimesKadabra = []\n",
    "for instance in sortedInstances:\n",
    "    logTimesRK.append(logTimes[algos[0]][instance.filename])\n",
    "    logTimesKadabra.append(logTimes[algos[1]][instance.filename])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define scaling model as T_{kadabra}(instance) = exp(\\alpha) * T_{rk}(instance)^{\\beta} + \\mathcal{N}(0, \\sigma)\n",
    "# Parameters have vague priors to reflect our initial ignorance\n",
    "basic_model = pm.Model()\n",
    "\n",
    "with basic_model:\n",
    "    \n",
    "    alpha = pm.Normal('alpha', mu=0, sd=10)\n",
    "    beta = pm.Normal('beta', mu=0, sd=10)\n",
    "    sigma = pm.InverseGamma('sigma', alpha=1, beta=1)\n",
    "\n",
    "    mu = alpha + beta*logTimesRK\n",
    "    \n",
    "    Y_obs = pm.Normal('Y_obs', mu=mu, sd=sigma, observed=logTimesKadabra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approximate posterior distribution of parameters with samples.\n",
    "# PyMC3 uses the Markov-Chain Monte Carlo approach, constructing a Markov chain with the same \n",
    "# stationary distribution as the desired posterior distribution of the parameters.\n",
    "# The number of required samples depends on the complexity of the model.\n",
    "\n",
    "from scipy import optimize\n",
    "\n",
    "with basic_model:\n",
    "    # draw 10000 posterior samples\n",
    "    trace = pm.sample(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the posterior distribution of parameters. \n",
    "# HPD_2.5 and HPD 97.5 designate the limits of an interval containing 95% of the probability mass\n",
    "pm.summary(trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graphical representation of the approximated distribution of parameters.\n",
    "# If the coloured lines do not match, this indicates that the distribution still depends on the starting value and \n",
    "# the markov chains have not converged sufficiently. In this case, more samples are needed.\n",
    "_ = pm.traceplot(trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayes Factor for Diameter Influence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get sorted logsizes\n",
    "logSizes = [math.log(len(exactValues[instance.filename])) for instance in sortedInstances]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get running times of kadabra, sorted by instance size.\n",
    "sortedLogTimesKadabra = []\n",
    "\n",
    "for instance in sortedInstances:\n",
    "    try:\n",
    "        sortedLogTimesKadabra.append(logTimes[algo][instance.filename])\n",
    "    except IndexError:\n",
    "        print(\"No run found for \" + algo + \" on \" + instance.filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the diameters are not stored with the instances, enter them manually from Table 1 in the paper\n",
    "\n",
    "diameters = dict()\n",
    "diameters['moreno_blogs'] = 8\n",
    "diameters['petster-hamster'] = 10\n",
    "diameters['ego-facebook'] = 9\n",
    "diameters['openflights'] = 13\n",
    "diameters['opsahl-powergrid'] = 46\n",
    "diameters['p2p-Gnutella08'] = 9\n",
    "diameters['advogato'] = 9\n",
    "diameters['wiki-Vote'] = 7\n",
    "diameters['p2p-Gnutella05'] = 9\n",
    "diameters['p2p-Gnutella04'] = 10\n",
    "diameters['foldoc'] = 8\n",
    "diameters['twin'] = 25\n",
    "diameters['cfinder-google'] = 7\n",
    "diameters['ca-AstroPh'] = 14\n",
    "diameters['ca-cit-HepTh'] = 9\n",
    "diameters['subelj_cora'] = 20\n",
    "diameters['ego-twitter'] = 15\n",
    "diameters['ego-gplus'] = 8\n",
    "diameters['p2p-Gnutella24'] = 11\n",
    "diameters['ca-cit-HepPh'] = 9\n",
    "diameters['cit-HepPh'] = 14\n",
    "diameters['dblp-cite'] = 2\n",
    "diameters['loc-brightkite_edges'] = 18\n",
    "diameters['edit-frwikinews'] = 7\n",
    "diameters['dimacs9-BAY'] = 837\n",
    "diameters['dimacs9-COL'] = 1255 \n",
    "diameters['roadNet-PA'] = 794\n",
    "diameters['roadNet-TX'] = 1064 \n",
    "diameters['facebook-wosn-wall'] = 18\n",
    "diameters['edit-frwikibooks'] = 12\n",
    "\n",
    "logDiameters = [math.log(diameters[instance.filename]) for instance in sortedInstances]\n",
    "assert(len(logDiameters) == len(logSizes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a hierarchical model comparing the fit of only T_RK vs T_RK+diameter to the running times.\n",
    "# The two models are:\n",
    "#     T_{kadabra}(instance) = exp(\\alpha) * T_{RK}(instance)^{\\beta} + \\mathcal{N}(0, \\sigma)\n",
    "#     T_{kadabra}(instance) = exp(\\alpha) * T_{RK}(instance)^{\\beta} + diameter(instance)^{\\gamma} + \\mathcal{N}(0, \\sigma)\n",
    "# \n",
    "# The difference is the addition of the diameter term.\n",
    "# Thus, multiplying this term with the boolean indicator variable selected_model serves to distinguish between the models.\n",
    "basic_model = pm.Model()\n",
    "\n",
    "with basic_model:\n",
    "    \n",
    "    pi = (0.5, 0.5)\n",
    "    \n",
    "    selected_model = pm.Bernoulli('selected_model', p=pi[1])\n",
    "\n",
    "    alpha = pm.Normal('alpha', mu=0, sd=1)\n",
    "    beta = pm.Normal('beta', mu=0, sd=1)\n",
    "    gamma = pm.Normal('gamma', mu=0, sd=1)\n",
    "    sigma = pm.InverseGamma('sigma', alpha=1,beta=1)\n",
    "    \n",
    "    mu = alpha + beta*logTimesRK + gamma*logDiameters*selected_model\n",
    "    \n",
    "    Y_obs = pm.Normal('Y_obs', mu=mu, sd=sigma, observed=sortedLogTimesKadabra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the model is more complex since the last one, we take more samples to get accurate estimates.\n",
    "with basic_model:\n",
    "    # draw 200000 posterior samples\n",
    "    trace = pm.sample(200000,tune=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print key values of posterior distribution\n",
    "pm.summary(trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Bayes Factor as posterior ratio times inverse prior ratio\n",
    "BF = ((1-pm.summary(trace)['mean']['selected_model'])/pm.summary(trace)['mean']['selected_model']) * (pi[1]/pi[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(BF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot traces.\n",
    "_ = pm.traceplot(trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
